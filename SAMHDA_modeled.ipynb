{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'first-planet-266901:samhda_modeled' successfully created.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US mk --dataset samhda_modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"samhda_modeled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new modeled tables using CTAS statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE samhda_modeled.facility_information AS\n",
    "SELECT generate_uuid() as CASEID, *\n",
    "FROM\n",
    "(SELECT DISTINCT LST, CAST(CASE WHEN MHINTAKE = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as MHINTAKE, SETTINGIP, SETTINGOP, FACILITYTYPE, OWNERSHP, CHILDAD,\n",
    "FROM samhda_staging. samhda_2018\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT LST, CAST(CASE WHEN MHINTAKE = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as MHINTAKE, SETTINGIP, SETTINGOP, FACILITYTYPE, OWNERSHP, CHILDAD,\n",
    "FROM samhda_staging. samhda_2017\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT LST, CAST(CASE WHEN MHINTAKE = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as MHINTAKE, SETTINGIP, SETTINGOP, FACILITYTYPE, OWNERSHP, CHILDAD,\n",
    "FROM samhda_staging. samhda_2016\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    "\n",
    "SELECT DISTINCT LST, CAST(CASE WHEN MHINTAKE = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as MHINTAKE, SETTINGIP, SETTINGOP, FACILITYTYPE, OWNERSHP, CHILDAD,\n",
    "FROM samhda_staging. samhda_2015\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    "\n",
    "SELECT DISTINCT LST, CAST(CASE WHEN MHINTAKE = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as MHINTAKE, SETTINGIP, SETTINGOP, FACILITYTYPE, OWNERSHP, CHILDAD,\n",
    "FROM samhda_staging. samhda_2014\n",
    "WHERE CASEID IS NOT NULL)\n",
    "\n",
    "\n",
    "\n",
    "ORDER BY LST;\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE samhda_modeled.facility_treatment AS\n",
    "SELECT generate_uuid() as CASEID, *\n",
    "FROM\n",
    "\n",
    "(SElECT DISTINCT CAST(CASE WHEN TREATMT = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as TREATMT, TREATGRPTHRPY, TREATCOGTHRPY, TREATTRAUMATHRPY, ALZHDEMENTIA, SRVC31, SPECGRPEATING, TRAUMATICBRAIN,\n",
    "FROM samhda_staging. samhda_2018\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SElECT DISTINCT CAST(CASE WHEN TREATMT = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as TREATMT, TREATGRPTHRPY, TREATCOGTHRPY, TREATTRAUMATHRPY, ALZHDEMENTIA, SRVC31, SPECGRPEATING, TRAUMATICBRAIN,\n",
    "FROM samhda_staging. samhda_2017\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SElECT DISTINCT CAST(CASE WHEN TREATMT = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as TREATMT, TREATGRPTHRPY, TREATCOGTHRPY, TREATTRAUMATHRPY, ALZHDEMENTIA, SRVC31, SPECGRPEATING, TRAUMATICBRAIN,\n",
    "FROM samhda_staging. samhda_2016\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SElECT DISTINCT CAST(CASE WHEN TREATMT = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as TREATMT, TREATGRPTHRPY, TREATCOGTHRPY, TREATTRAUMATHRPY, ALZHDEMENTIA, SRVC31, SPECGRPEATING, TRAUMATICBRAIN,\n",
    "FROM samhda_staging. samhda_2015\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SElECT DISTINCT CAST(CASE WHEN TREATMT = 1 THEN 'Yes' WHEN MHINTAKE = 0 THEN 'No' ELSE Null END as String) as TREATMT, TREATGRPTHRPY, TREATCOGTHRPY, TREATTRAUMATHRPY, ALZHDEMENTIA, SRVC31, SPECGRPEATING, TRAUMATICBRAIN,\n",
    "FROM samhda_staging. samhda_2014\n",
    "WHERE CASEID IS NOT NULL)\n",
    "\n",
    "\n",
    " \n",
    "ORDER BY CASEID;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE samhda_modeled.facility_services AS\n",
    "SELECT generate_uuid() as CASEID, *\n",
    "FROM\n",
    "\n",
    "(SELECT DISTINCT MHINTCASEMGMT, CAST(CASE WHEN MHCHRONIC = 1 THEN 'Yes' WHEN MHCHRONIC = 0 THEN 'No' ELSE Null END as String) as MHCHRONIC, ILLNESSMGMT, PRIMARYCARE,FAMPSYCHED, SUPPHOUSING, SUPPEMPLOY, MHLEGAL, MHEMGCY, MHSUICIDE\n",
    "FROM samhda_staging. samhda_2018\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT MHINTCASEMGMT, CAST(CASE WHEN MHCHRONIC = 1 THEN 'Yes' WHEN MHCHRONIC = 0 THEN 'No' ELSE Null END as String) as MHCHRONIC, ILLNESSMGMT, PRIMARYCARE,FAMPSYCHED, SUPPHOUSING, SUPPEMPLOY, MHLEGAL, MHEMGCY, MHSUICIDE\n",
    "FROM samhda_staging. samhda_2017\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT MHINTCASEMGMT, CAST(CASE WHEN MHCHRONIC = 1 THEN 'Yes' WHEN MHCHRONIC = 0 THEN 'No' ELSE Null END as String) as MHCHRONIC, ILLNESSMGMT, PRIMARYCARE,FAMPSYCHED, SUPPHOUSING, SUPPEMPLOY, MHLEGAL, MHEMGCY, MHSUICIDE\n",
    "FROM samhda_staging. samhda_2016\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT MHINTCASEMGMT, CAST(CASE WHEN MHCHRONIC = 1 THEN 'Yes' WHEN MHCHRONIC = 0 THEN 'No' ELSE Null END as String) as MHCHRONIC, ILLNESSMGMT, PRIMARYCARE,FAMPSYCHED, SUPPHOUSING, SUPPEMPLOY, MHLEGAL, MHEMGCY, MHSUICIDE\n",
    "FROM samhda_staging. samhda_2015\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT MHINTCASEMGMT, CAST(CASE WHEN MHCHRONIC = 1 THEN 'Yes' WHEN MHCHRONIC = 0 THEN 'No' ELSE Null END as String) as MHCHRONIC, ILLNESSMGMT, PRIMARYCARE,FAMPSYCHED, SUPPHOUSING, SUPPEMPLOY, MHLEGAL, MHEMGCY, MHSUICIDE\n",
    "FROM samhda_staging. samhda_2015\n",
    "WHERE CASEID IS NOT NULL)\n",
    "\n",
    "ORDER BY CASEID;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Primary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f0_\n",
       "0  4359"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f0_\n",
       "0  4359"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  540"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  540"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  792"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  792"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'samhda_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_information'> referenced by query SELECT CASEID, LST, MHINTAKE, SETTINGIP, SETTINGOP, FACILITYTYPE,OWNERSHP,CHILDAD FROM samhda_modeled.facility_information limit 20\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset first-planet-266901:temp_dataset_c42ad9cba2584c0d9aa3b6e4bfce62d4 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table first-planet-266901.samhda_modeled.facility_information_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'LST'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHINTAKE'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SETTINGIP'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SETTINGOP'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'FACILITYTYPE'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'OWNERSHP'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CHILDAD'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583260753974\n",
      " etag: 'yyvGGQz0qNyhZDa8woGWCA=='\n",
      " id: 'first-planet-266901:samhda_modeled.facility_information_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583260754010\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'LST'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHINTAKE'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SETTINGIP'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SETTINGOP'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'FACILITYTYPE'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'OWNERSHP'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CHILDAD'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/first-planet-266901/datasets/samhda_modeled/tables/facility_information_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'samhda_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_information_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "%run facility_information_beam.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_information_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_information_Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# milestone 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'samhda_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_treatment'> referenced by query SELECT CASEID, TREATMT, TREATGRPTHRPY, TREATTRAUMATHRPY, ALZHDEMENTIA, SRVC31,SPECGRPEATING,TRAUMATICBRAIN FROM samhda_modeled.facility_treatment limit 20\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset first-planet-266901:temp_dataset_04d6fc37d926480c81c6098f8eba21e4 does not exist so we will create it as temporary with location=US\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f45a0e4e108>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: 'MHINTCASEMGMT'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 381, in call\n",
      "    finish_state)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 418, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/transform_evaluator.py\", line 740, in process_element\n",
      "    self.runner.process(element)\n",
      "  File \"apache_beam/runners/common.py\", line 880, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/future/utils/__init__.py\", line 446, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: \"MHINTCASEMGMT [while running 'Format Data']\"\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f45a0e4e108>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: 'MHINTCASEMGMT'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 381, in call\n",
      "    finish_state)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 418, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/transform_evaluator.py\", line 740, in process_element\n",
      "    self.runner.process(element)\n",
      "  File \"apache_beam/runners/common.py\", line 880, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/future/utils/__init__.py\", line 446, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: \"MHINTCASEMGMT [while running 'Format Data']\"\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f45a0e4e108>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: 'MHINTCASEMGMT'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 381, in call\n",
      "    finish_state)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 418, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/transform_evaluator.py\", line 740, in process_element\n",
      "    self.runner.process(element)\n",
      "  File \"apache_beam/runners/common.py\", line 880, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/future/utils/__init__.py\", line 446, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: \"MHINTCASEMGMT [while running 'Format Data']\"\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f45a0e4e108>, due to an exception.\n",
      " Traceback (most recent call last):\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: 'MHINTCASEMGMT'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 381, in call\n",
      "    finish_state)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\", line 418, in attempt_call\n",
      "    evaluator.process_element(value)\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/transform_evaluator.py\", line 740, in process_element\n",
      "    self.runner.process(element)\n",
      "  File \"apache_beam/runners/common.py\", line 880, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 885, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 956, in apache_beam.runners.common.DoFnRunner._reraise_augmented\n",
      "  File \"/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/future/utils/__init__.py\", line 446, in raise_with_traceback\n",
      "    raise exc.with_traceback(traceback)\n",
      "  File \"apache_beam/runners/common.py\", line 883, in apache_beam.runners.common.DoFnRunner.process\n",
      "  File \"apache_beam/runners/common.py\", line 498, in apache_beam.runners.common.SimpleInvoker.invoke_process\n",
      "  File \"/home/jupyter/Team-Foodies/facility_services_beam.py\", line 43, in process\n",
      "    'MHINTCASEMGMT': data['MHINTCASEMGMT'],\n",
      "KeyError: \"MHINTCASEMGMT [while running 'Format Data']\"\n",
      "\n",
      "ERROR:apache_beam.runners.direct.executor:Giving up after 4 attempts.\n",
      "WARNING:apache_beam.runners.direct.executor:A task failed with exception: \"MHINTCASEMGMT [while running 'Format Data']\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"MHINTCASEMGMT [while running 'Format Data']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/common.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/common.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/facility_services_beam.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;34m'CASEID'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CASEID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;34m'MHINTCASEMGMT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MHINTCASEMGMT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;34m'MHCHRONIC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MHCHRONIC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MHINTCASEMGMT'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/jupyter/Team-Foodies/facility_treatment_beam.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/facility_treatment_beam.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m      \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m      \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/direct_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m    484\u001b[0m             'DirectRunner does not support duration argument.')\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\u001b[0m in \u001b[0;36mawait_completion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mawait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawait_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\u001b[0m in \u001b[0;36mawait_completion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mraise_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/future/utils/__init__.py\u001b[0m in \u001b[0;36mraise_\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEllipsis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, state_sampler)\u001b[0m\n\u001b[1;32m    379\u001b[0m                           \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                           \u001b[0mprocess_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                           finish_state)\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/executor.py\u001b[0m in \u001b[0;36mattempt_call\u001b[0;34m(self, metrics_container, side_input_values, start_state, process_state, finish_state)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_bundle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_elements_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m           \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfinish_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/direct/transform_evaluator.py\u001b[0m in \u001b[0;36mprocess_element\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfinish_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/common.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/common.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/common.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/future/utils/__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[0;34m(exc, traceback)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/common.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/common.cpython-35m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/Team-Foodies/facility_services_beam.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m     41\u001b[0m         return [{\n\u001b[1;32m     42\u001b[0m             \u001b[0;34m'CASEID'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CASEID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;34m'MHINTCASEMGMT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MHINTCASEMGMT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;34m'MHCHRONIC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MHCHRONIC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;34m'ILLNESSMGMT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ILLNESSMGMT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"MHINTCASEMGMT [while running 'Format Data']\""
     ]
    }
   ],
   "source": [
    "%run facility_treatment_beam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'samhda_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_services'> referenced by query SELECT CASEID, MHINTCASEMGMT, MHCHRONIC, ILLNESSMGMT, PRIMARYCARE, FAMPSYCHED,SUPPHOUSING,SUPPEMPLOY, MHLEGAL, MHEMGCY, MHSUICIDE FROM samhda_modeled.facility_services limit 20\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset first-planet-266901:temp_dataset_c7d00ed0c0ad43719ff8604ad46a5a8a does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table first-planet-266901.samhda_modeled.facility_services_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHINTCASEMGMT'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHCHRONIC'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'ILLNESSMGMT'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'PRIMARYCARE'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'FAMPSYCHED'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SUPPHOUSING'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SUPPEMPLOY'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHLEGAL'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHEMGCY'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHSUICIDE'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1583643311556\n",
      " etag: 'glnvFC68D0hkAPQWDFfhkg=='\n",
      " id: 'first-planet-266901:samhda_modeled.facility_services_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1583643311587\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHINTCASEMGMT'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHCHRONIC'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'ILLNESSMGMT'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'PRIMARYCARE'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'FAMPSYCHED'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SUPPHOUSING'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SUPPEMPLOY'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHLEGAL'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHEMGCY'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'MHSUICIDE'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/first-planet-266901/datasets/samhda_modeled/tables/facility_services_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'samhda_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_services_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "%run facility_services_beam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/pipeline.pb...\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 2/2)\n",
      "INFO:oauth2client.transport:Refreshing due to a 401 (attempt 1/2)\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpxjxinqv7', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpxjxinqv7', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-information.1583714708.759256/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T00:45:16.590961Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-08_17_45_15-11919932638966063853'\n",
      " location: 'us-central1'\n",
      " name: 'transform-facility-information'\n",
      " projectId: 'first-planet-266901'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T00:45:16.590961Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-08_17_45_15-11919932638966063853]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-08_17_45_15-11919932638966063853?project=first-planet-266901\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_17_45_15-11919932638966063853 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:15.498Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-08_17_45_15-11919932638966063853. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:15.499Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-08_17_45_15-11919932638966063853.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:18.689Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:20.151Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-4 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:20.798Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:20.828Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log 1/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:20.863Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:20.892Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:20.926Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.021Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.322Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.356Z: JOB_MESSAGE_DETAILED: Fusing consumer Format Data into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.393Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.426Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WriteBundles/WriteBundles into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.459Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.490Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Pair into Write log input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.525Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.563Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Reify into Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.598Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Write into Write log input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.624Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/GroupByWindow into Write log input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.650Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Extract into Write log input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.681Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Pair into Write log 1/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.708Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log 1/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.773Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Reify into Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.807Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Write into Write log 1/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.845Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow into Write log 1/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.880Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Extract into Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.915Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/InitializeWrite into Write log input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.952Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/InitializeWrite into Write log 1/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:21.992Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.026Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.048Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.074Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.276Z: JOB_MESSAGE_DEBUG: Executing wait step start29\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.328Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.360Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.371Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.391Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.401Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.419Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.451Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.474Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.524Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:22.557Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_17_45_15-11919932638966063853 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:45:47.935Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:06.709Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:06.736Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.415Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.465Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.489Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.543Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.570Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.582Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.594Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.634Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.643Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.664Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.682Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:35.709Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.645Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.749Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.776Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.825Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.848Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.873Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.889Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.899Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.958Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:36.978Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:37.092Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:37.122Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:37.158Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:47:38.196Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_610196024193168093\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_610196024193168093\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:48:42.976Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_610196024193168093\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:48:43.454Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_6440175454286151789\" started. You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_6440175454286151789\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:13.844Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_6440175454286151789\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:13.869Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_6440175454286151789\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:17.108Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_610196024193169033\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_610196024193169033\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:27.552Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_610196024193169033\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:28.524Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:28.572Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:28.600Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:28.615Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:28.639Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:28.658Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:28.688Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:31.948Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.009Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.066Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.092Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.115Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.131Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.171Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.201Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:32.252Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.018Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.071Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.123Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.149Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.169Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.203Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.221Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.250Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.306Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.840Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.889Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.941Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:35.982Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:36.030Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:36.088Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:38.794Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:39.032Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:39.085Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:39.134Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:39.185Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:39.244Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:39.291Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:41.575Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:41.630Z: JOB_MESSAGE_DEBUG: Executing success step success27\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:41.732Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:41.784Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:49:41.813Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:51:05.782Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:51:05.824Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T00:51:05.856Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_17_45_15-11919932638966063853 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run facility_information_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpjl5xkx67', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpjl5xkx67', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-treatment.1583728161.809824/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T04:29:27.362476Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-08_21_29_26-1291586958911727880'\n",
      " location: 'us-central1'\n",
      " name: 'transform-facility-treatment'\n",
      " projectId: 'first-planet-266901'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T04:29:27.362476Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-08_21_29_26-1291586958911727880]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-08_21_29_26-1291586958911727880?project=first-planet-266901\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_21_29_26-1291586958911727880 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:26.378Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-08_21_29_26-1291586958911727880.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:26.378Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-08_21_29_26-1291586958911727880. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:30.262Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:30.847Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-4 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.350Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.388Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log 1/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.419Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.453Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.487Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.580Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.813Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.848Z: JOB_MESSAGE_DETAILED: Fusing consumer Format Data into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.888Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.922Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WriteBundles/WriteBundles into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.952Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:31.987Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Pair into Write log input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.022Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.058Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Reify into Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.088Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Write into Write log input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.120Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/GroupByWindow into Write log input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.150Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Extract into Write log input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.184Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Pair into Write log 1/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.247Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log 1/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.280Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Reify into Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.319Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Write into Write log 1/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.363Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow into Write log 1/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.458Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Extract into Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.530Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/InitializeWrite into Write log input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.589Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/InitializeWrite into Write log 1/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.714Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.786Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.844Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:32.896Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.555Z: JOB_MESSAGE_DEBUG: Executing wait step start29\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.621Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.653Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.666Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.681Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.692Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.714Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.739Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.769Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.805Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:33.844Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_21_29_26-1291586958911727880 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:29:58.574Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:22.162Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:22.199Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:52.748Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:52.828Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:52.890Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:52.960Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:52.992Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.016Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.028Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.049Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.088Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.091Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.121Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.145Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.571Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.644Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.680Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.741Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.775Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.799Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.804Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.823Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.849Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.862Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.897Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.933Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:53.968Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:31:54.867Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_5158183863832383433\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_5158183863832383433\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:07.566Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_5158183863832383433\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:08.045Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_2916235241429907669\" started. You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_2916235241429907669\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:38.435Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_2916235241429907669\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:38.476Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_2916235241429907669\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:41.581Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_5158183863832382205\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_5158183863832382205\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:52.523Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_5158183863832382205\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:53.041Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:53.121Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:53.155Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:53.172Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:53.210Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:53.261Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:53.296Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:56.458Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:56.572Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:56.749Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:56.841Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:56.864Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:56.949Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:57.083Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:57.153Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:57.285Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:59.529Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:59.588Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:59.659Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:59.748Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:59.819Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:33:59.887Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:34:02.105Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:35:33.404Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:55.715Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:55.820Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:55.932Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:55.950Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:55.984Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:56.002Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:56.038Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:56.066Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:56.116Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:59.071Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:59.128Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:59.184Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:59.225Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:59.271Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:36:59.329Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:37:02.573Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:37:02.633Z: JOB_MESSAGE_DEBUG: Executing success step success27\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:37:02.758Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:37:02.825Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:37:02.857Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:38:38.869Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:38:38.912Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T04:38:38.944Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_21_29_26-1291586958911727880 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run facility_treatment_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpr3v9k2mu', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/dataflow_python_sdk.tar in 1 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpr3v9k2mu', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-facility-services.1583725837.190616/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-03-09T03:50:43.460862Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-03-08_20_50_42-12412035191624303622'\n",
      " location: 'us-central1'\n",
      " name: 'transform-facility-services'\n",
      " projectId: 'first-planet-266901'\n",
      " stageStates: []\n",
      " startTime: '2020-03-09T03:50:43.460862Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-03-08_20_50_42-12412035191624303622]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-03-08_20_50_42-12412035191624303622?project=first-planet-266901\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_20_50_42-12412035191624303622 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:42.265Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-03-08_20_50_42-12412035191624303622.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:42.265Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-03-08_20_50_42-12412035191624303622. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:46.170Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:46.919Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-4 in us-central1-f.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:47.556Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:47.588Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log 1/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:47.618Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:47.650Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:47.682Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:47.776Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.032Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.060Z: JOB_MESSAGE_DETAILED: Fusing consumer Format Data into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.086Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.110Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WriteBundles/WriteBundles into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.142Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.165Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Pair into Write log input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.188Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.214Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Reify into Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.239Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Write into Write log input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.268Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/GroupByWindow into Write log input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.297Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Extract into Write log input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.329Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Pair into Write log 1/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.356Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log 1/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.383Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Reify into Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.415Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Write into Write log 1/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.448Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow into Write log 1/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.478Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Extract into Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.502Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/InitializeWrite into Write log input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.528Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/InitializeWrite into Write log 1/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.556Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.585Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.608Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.638Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_20_50_42-12412035191624303622 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:48.908Z: JOB_MESSAGE_DEBUG: Executing wait step start29\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.045Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.098Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.114Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.129Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.146Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-f...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.159Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.206Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.206Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.269Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:50:49.294Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:51:13.838Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running step(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:29.062Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:29.091Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.639Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.670Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.713Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.744Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.776Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.802Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.833Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.858Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.887Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.892Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.915Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.925Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.946Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.962Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.970Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.972Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:57.998Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.001Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.027Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.029Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.060Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.091Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.117Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.142Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:58.169Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:52:59.465Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_2920764432082032421\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_2920764432082032421\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:08.202Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_2920764432082032421\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:08.649Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_18337795946319766414\" started. You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_18337795946319766414\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:39.011Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_18337795946319766414\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:39.039Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_18337795946319766414\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:42.693Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_2920764432082033953\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_2920764432082033953\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:53.138Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_2920764432082033953\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:53.803Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:53.870Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:53.902Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:53.915Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:53.947Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:53.973Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:54.014Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.259Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.306Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.367Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.394Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.431Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.441Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.513Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.593Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:57.708Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.623Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.691Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.743Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.773Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.789Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.816Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.844Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.874Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:54:59.921Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:00.807Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:00.971Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:01.051Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:01.121Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:01.186Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:01.246Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:02.727Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:02.782Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:02.835Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:02.878Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:02.935Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:02.992Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:03.337Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:04.127Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:04.175Z: JOB_MESSAGE_DEBUG: Executing success step success27\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:05.270Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:05.325Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:55:05.354Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:56:16.783Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:56:16.825Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-03-09T03:56:16.857Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-03-08_20_50_42-12412035191624303622 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run facility_services_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking primary keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  540"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_treatment_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  540"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_treatment_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_treatment_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_treatment_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  792"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_services_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0  792"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_services_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_services_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_services_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f0_\n",
       "0  4359"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from samhda_modeled.facility_information_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f0_\n",
       "0  4359"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from samhda_modeled.facility_information_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
