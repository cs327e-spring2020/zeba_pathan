{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'first-planet-266901:ssats_modeled' successfully created.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"ssats_modeled\"\n",
    "!bq --location=US mk --dataset {dataset_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new modeled tables using CTAS statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE ssats_modeled.facility_information AS\n",
    "SELECT *\n",
    "FROM\n",
    "(SELECT DISTINCT CASEID, STATE, CAST(CASE WHEN CTYPEHI2 = 1 THEN 'Yes' WHEN CTYPEHI2 = 0 THEN 'No' ELSE Null END as String) as CTYPEHI2, CTYPE1, OWNE_2SHP AS OWNERSHP,\n",
    "FROM ssats_staging. ssats_2018_u\n",
    "WHERE CASEI_3 IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, STATE, CAST(CASE WHEN CTYPEHI2 = 1 THEN 'Yes' WHEN CTYPEHI2 = 0 THEN 'No' ELSE Null END as String) as CTYPEHI2, CTYPE1, OW_5E_2SHP AS OWNERSHP,\n",
    "FROM ssats_staging. ssats_2017_u\n",
    "WHERE CASEI_3 IS NOT NULL\n",
    "UNION DISTINCT\n",
    "\n",
    "SELECT DISTINCT CASEID, STATE, CAST(CASE WHEN CTYPEHI2 = 1 THEN 'Yes' WHEN CTYPEHI2 = 0 THEN 'No' ELSE Null END as String) as CTYPEHI2, CTYPE1, OWNE_2SHP AS OWNERSHP,\n",
    "FROM ssats_staging. ssats_2016_u\n",
    "WHERE CASEI_3 IS NOT NULL\n",
    "UNION DISTINCT \n",
    " \n",
    "SELECT DISTINCT CASEID, STATE, CAST(CASE WHEN CTYPEHI2 = 1 THEN 'Yes' WHEN CTYPEHI2 = 0 THEN 'No' ELSE Null END as String) as CTYPEHI2, CTYPE1, OWNE_2SHP AS OWNERSHP,\n",
    "FROM ssats_staging. ssats_2015_u\n",
    "WHERE CASEI_3 IS NOT NULL\n",
    "UNION DISTINCT \n",
    " \n",
    "SELECT DISTINCT CASEID, STATE, CAST(CASE WHEN CTYPEHI2 = 1 THEN 'Yes' WHEN CTYPEHI2 = 0 THEN 'No' ELSE Null END as String) as CTYPEHI2, CTYPE1, OWNE_2SHP AS OWNERSHP,\n",
    "FROM ssats_staging. ssats_2014_u\n",
    "WHERE CASEI_3 IS NOT NULL)\n",
    "\n",
    "\n",
    "ORDER BY STATE;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE ssats_modeled.facility_treatment AS\n",
    "SELECT *\n",
    "FROM\n",
    "(SELECT DISTINCT CASEID, OTP, CAST(CASE WHEN T_2EAT_1T = 1 THEN 'Yes' WHEN T_2EAT_1T = 0 THEN 'No' ELSE Null END as String) as TREATMT, S_2VC71 AS SRVC71, CTYPEHI1, CTYPE_2C1 AS CTYPERC1, S_2VC85 AS SRVC85,\n",
    "FROM ssats_staging. ssats_2018_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, OTP, CAST(CASE WHEN T_2EAT_1T = 1 THEN 'Yes' WHEN T_2EAT_1T = 0 THEN 'No' ELSE Null END as String) as TREATMT, S_2VC71 AS SRVC71, CTYPEHI1, CTYPE_2C1 AS CTYPERC1, S_2VC85 AS SRVC85,\n",
    "FROM ssats_staging. ssats_2017_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, OTP, CAST(CASE WHEN T_2EAT_1T = 1 THEN 'Yes' WHEN T_2EAT_1T = 0 THEN 'No' ELSE Null END as String) as TREATMT, S_2VC71 AS SRVC71, CTYPEHI1, CTYPE_2C1 AS CTYPERC1, S_2VC85 AS SRVC85,\n",
    "FROM ssats_staging. ssats_2016_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, OTP, CAST(CASE WHEN T_2EAT_1T = 1 THEN 'Yes' WHEN T_2EAT_1T = 0 THEN 'No' ELSE Null END as String) as TREATMT, S_2VC71 AS SRVC71, CTYPEHI1, CTYPE_2C1 AS CTYPERC1, S_2VC85 AS SRVC85,\n",
    "FROM ssats_staging. ssats_2015_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, OTP, CAST(CASE WHEN T_2EAT_1T = 1 THEN 'Yes' WHEN T_2EAT_1T = 0 THEN 'No' ELSE Null END as String) as TREATMT, S_2VC71 AS SRVC71, CTYPEHI1, CTYPE_2C1 AS CTYPERC1, S_2VC85 AS SRVC85,\n",
    "FROM ssats_staging. ssats_2014_u\n",
    "WHERE CASEID IS NOT NULL)\n",
    "\n",
    "ORDER BY CASEID;\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE TABLE ssats_modeled.facility_services AS\n",
    "SELECT *\n",
    "FROM\n",
    "(SELECT DISTINCT CASEID, CAST(CASE WHEN SIGNLANG = 1 THEN 'Yes' WHEN SIGNLANG = 0 THEN 'No' ELSE Null END as String) as SIGNLANG, CTYPE_1L AS CTYPEML, LANG16, S_2VC108 AS SRVC108,S_2VC113 AS SRVC113,\n",
    "FROM ssats_staging. ssats_2018_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID,CAST(CASE WHEN SIG_5LA_5G = 1 THEN 'Yes' WHEN SIG_5LA_5G = 0 THEN 'No' ELSE Null END as String) as SIGNLANG, CTYPE_1L AS CTYPEML, LA_5G16 AS LANG16, S_2VC108 AS SRVC108,S_2VC113 AS SRVC113,\n",
    "FROM ssats_staging. ssats_2017_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, CAST(CASE WHEN SIGNLANG = 1 THEN 'Yes' WHEN SIGNLANG = 0 THEN 'No' ELSE Null END as String) as SIGNLANG, CTYPE_1L AS CTYPEML, LANG16, S_2VC108 AS SRVC108,S_2VC113 AS SRVC113,\n",
    "FROM ssats_staging. ssats_2016_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, CAST(CASE WHEN SIGNLANG = 1 THEN 'Yes' WHEN SIGNLANG = 0 THEN 'No' ELSE Null END as String) as SIGNLANG, CTYPE_1L AS CTYPEML, LANG16, S_2VC108 AS SRVC108,S_2VC113 AS SRVC113,\n",
    "FROM ssats_staging. ssats_2015_u\n",
    "WHERE CASEID IS NOT NULL\n",
    "UNION DISTINCT\n",
    " \n",
    "SELECT DISTINCT CASEID, CAST(CASE WHEN SIGNLANG = 1 THEN 'Yes' WHEN SIGNLANG = 0 THEN 'No' ELSE Null END as String) as SIGNLANG, CTYPE_1L AS CTYPEML, LANG16, S_2VC108 AS SRVC108, S_2VC113 AS SRVC113,\n",
    "FROM ssats_staging. ssats_2014_u\n",
    "WHERE CASEID IS NOT NULL)\n",
    "\n",
    "ORDER BY CASEID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.facility_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.facility_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.facility_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.facility_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.facility_services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.facility_services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.facility_information as t2 \n",
    "LEFT JOIN ssats_modeled.facility_services as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.facility_services as t2 \n",
    "LEFT JOIN ssats_modeled.facility_information as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.facility_services as t2 \n",
    "LEFT JOIN ssats_modeled.facility_treatment as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.facility_treatment as t2 \n",
    "LEFT JOIN ssats_modeled.facility_services as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.facility_treatment as t2 \n",
    "LEFT JOIN ssats_modeled.facility_information as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Transformations (Direct Runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Interactive Beam requires Python 3.5.3+.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'ssats_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_information'> referenced by query SELECT CASEID, STATE, CTYPEHI2, CTYPE1,OWNERSHP FROM ssats_modeled.facility_information limit 20\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset first-planet-266901:temp_dataset_3e80c8b847e04b759c14af9128f76a91 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table first-planet-266901.ssats_modeled.ssats_facility_information_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'STATE'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPEHI2'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPE1'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'OWNERSHP'\n",
      " type: 'INTEGER'>]>. Result: <Table\n",
      " creationTime: 1587931862619\n",
      " etag: 'R9G0U4J92OIFqcpeZGNGTQ=='\n",
      " id: 'first-planet-266901:ssats_modeled.ssats_facility_information_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1587931862655\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'STATE'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPEHI2'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPE1'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'OWNERSHP'\n",
      " type: 'INTEGER'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/first-planet-266901/datasets/ssats_modeled/tables/ssats_facility_information_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'ssats_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'ssats_facility_information_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.11 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "%run ssats_facility_information_beam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Interactive Beam requires Python 3.5.3+.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'ssats_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_services'> referenced by query SELECT CASEID, SIGNLANG, CTYPEML, LANG16, SRVC108,SRVC113 FROM ssats_modeled.facility_services limit 20\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset first-planet-266901:temp_dataset_457db6f57a3e473e85ce84c089f06db1 does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table first-planet-266901.ssats_modeled.ssats_facility_services_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SIGNLANG'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPEML'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'LANG16'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC108'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC113'\n",
      " type: 'STRING'>]>. Result: <Table\n",
      " creationTime: 1587775221923\n",
      " etag: 'cPcDnPMSfUA2o1cz7CLQBQ=='\n",
      " id: 'first-planet-266901:ssats_modeled.ssats_facility_services_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1587775221953\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SIGNLANG'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPEML'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'LANG16'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC108'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC113'\n",
      " type: 'STRING'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/first-planet-266901/datasets/ssats_modeled/tables/ssats_facility_services_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'ssats_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'ssats_facility_services_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "%run ssats_facility_services_beam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Interactive Beam requires Python 3.5.3+.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/io/gcp/bigquery.py:1421: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "INFO:apache_beam.runners.direct.direct_runner:Running pipeline with DirectRunner.\n",
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Using location 'US' from table <TableReference\n",
      " datasetId: 'ssats_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'facility_treatment'> referenced by query SELECT CASEID,TREATMT, SRVC71, OTP, CTYPEHI1,CTYPERC1,SRVC85 FROM ssats_modeled.facility_treatment limit 20\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset first-planet-266901:temp_dataset_33ae5fb07d164c98b743bde2c755722b does not exist so we will create it as temporary with location=US\n",
      "INFO:apache_beam.io.gcp.bigquery_tools:Created table first-planet-266901.ssats_modeled.ssats_facility_treatment_Beam with schema <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'TREATMT'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC71'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'OTP'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPEHI1'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPERC1'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC85'\n",
      " type: 'STRING'>]>. Result: <Table\n",
      " creationTime: 1587775644296\n",
      " etag: 'DzJkbbzwlyLkmVkbThT+OQ=='\n",
      " id: 'first-planet-266901:ssats_modeled.ssats_facility_treatment_Beam'\n",
      " kind: 'bigquery#table'\n",
      " lastModifiedTime: 1587775644341\n",
      " location: 'US'\n",
      " numBytes: 0\n",
      " numLongTermBytes: 0\n",
      " numRows: 0\n",
      " schema: <TableSchema\n",
      " fields: [<TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CASEID'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'TREATMT'\n",
      " type: 'STRING'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC71'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'OTP'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPEHI1'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'CTYPERC1'\n",
      " type: 'INTEGER'>, <TableFieldSchema\n",
      " fields: []\n",
      " mode: 'NULLABLE'\n",
      " name: 'SRVC85'\n",
      " type: 'STRING'>]>\n",
      " selfLink: 'https://www.googleapis.com/bigquery/v2/projects/first-planet-266901/datasets/ssats_modeled/tables/ssats_facility_treatment_Beam'\n",
      " tableReference: <TableReference\n",
      " datasetId: 'ssats_modeled'\n",
      " projectId: 'first-planet-266901'\n",
      " tableId: 'ssats_facility_treatment_Beam'>\n",
      " type: 'TABLE'>.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "%run ssats_facility_treatment_beam.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check primary key for direct runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.ssats_facility_treatment_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.ssats_facility_treatment_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.ssats_facility_services_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.ssats_facility_services_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.ssats_facility_information_Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0   20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.ssats_facility_information_Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Beam DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp14zafl2c', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp14zafl2c', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-services.1587846178.521949/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-04-25T20:23:03.966728Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-04-25_13_23_02-3935130524090949056'\n",
      " location: 'us-central1'\n",
      " name: 'transform-ssats-facility-services'\n",
      " projectId: 'first-planet-266901'\n",
      " stageStates: []\n",
      " startTime: '2020-04-25T20:23:03.966728Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-04-25_13_23_02-3935130524090949056]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-04-25_13_23_02-3935130524090949056?project=first-planet-266901\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-25_13_23_02-3935130524090949056 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:03.012Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-04-25_13_23_02-3935130524090949056. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:03.012Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-04-25_13_23_02-3935130524090949056.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:05.947Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:06.530Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-4 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.049Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.073Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log 1/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.093Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.125Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.155Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.221Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.460Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.489Z: JOB_MESSAGE_DETAILED: Fusing consumer Format Data into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.519Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.545Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WriteBundles/WriteBundles into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.575Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.604Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Pair into Write log input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.629Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.653Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Reify into Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.680Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Write into Write log input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.703Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/GroupByWindow into Write log input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.730Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Extract into Write log input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.755Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Pair into Write log 1/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.784Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log 1/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.805Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Reify into Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.832Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Write into Write log 1/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.967Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow into Write log 1/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:07.995Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Extract into Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.025Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/InitializeWrite into Write log input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.051Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/InitializeWrite into Write log 1/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.082Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.103Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.130Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.164Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.405Z: JOB_MESSAGE_DEBUG: Executing wait step start29\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.463Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.497Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.509Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.527Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.531Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.549Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.584Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.584Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.639Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:08.667Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-25_13_23_02-3935130524090949056 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:23:38.634Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:24:49.170Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:24:49.197Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.431Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.482Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.513Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.563Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.587Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.590Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.599Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.622Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.632Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.646Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.662Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.676Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.700Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.733Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.762Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.790Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.819Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.837Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.850Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.864Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.896Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.906Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.928Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.957Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:17.986Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:25:18.722Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_9605779912477477806\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_9605779912477477806\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:26:23.992Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_9605779912477477806\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:26:24.526Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_10826274656708343257\" started. You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_10826274656708343257\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:26:54.850Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_10826274656708343257\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:26:54.875Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_10826274656708343257\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:00.929Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_9605779912477477314\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_9605779912477477314\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:11.456Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_9605779912477477314\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:11.867Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:11.930Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:11.956Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:11.972Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:11.999Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:12.024Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:12.052Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:14.959Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.007Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.061Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.086Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.106Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.136Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.164Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.190Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:15.246Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.248Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.305Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.351Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.385Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.400Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.433Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.455Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.484Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:16.544Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:17.895Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:17.947Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.001Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.048Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.100Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.154Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.616Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.674Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.727Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.787Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.839Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:18.892Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:21.200Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:22.377Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:22.436Z: JOB_MESSAGE_DEBUG: Executing success step success27\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:22.539Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:22.585Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:27:22.603Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:29:08.950Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:29:08.988Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T20:29:09.015Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-25_13_23_02-3935130524090949056 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run ssats_facility_services_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpepqwyc3m', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpepqwyc3m', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-treatment.1587850571.800215/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-04-25T21:36:17.351017Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-04-25_14_36_16-8956075076107287369'\n",
      " location: 'us-central1'\n",
      " name: 'transform-ssats-facility-treatment'\n",
      " projectId: 'first-planet-266901'\n",
      " stageStates: []\n",
      " startTime: '2020-04-25T21:36:17.351017Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-04-25_14_36_16-8956075076107287369]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-04-25_14_36_16-8956075076107287369?project=first-planet-266901\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-25_14_36_16-8956075076107287369 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:16.370Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-04-25_14_36_16-8956075076107287369.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:16.370Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-04-25_14_36_16-8956075076107287369. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:19.732Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:20.220Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-4 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:20.753Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:20.829Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log 1/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:20.851Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:20.886Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:20.915Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:20.984Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.170Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.194Z: JOB_MESSAGE_DETAILED: Fusing consumer Format Data into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.219Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.248Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WriteBundles/WriteBundles into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.278Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.297Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Pair into Write log input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.326Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.346Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Reify into Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.378Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Write into Write log input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.401Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/GroupByWindow into Write log input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.424Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Extract into Write log input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.453Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Pair into Write log 1/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.476Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log 1/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.506Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Reify into Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.528Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Write into Write log 1/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.554Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow into Write log 1/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.583Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Extract into Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.613Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/InitializeWrite into Write log input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.638Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/InitializeWrite into Write log 1/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.666Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.695Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.720Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:21.746Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.113Z: JOB_MESSAGE_DEBUG: Executing wait step start29\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.175Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.197Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.209Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.223Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.239Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.249Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.296Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.296Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.351Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:22.379Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-25_14_36_16-8956075076107287369 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:36:49.613Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:12.244Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:12.270Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.413Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.490Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.526Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.603Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.631Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.649Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.668Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.684Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.712Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.721Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.750Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:39.783Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.251Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.320Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.354Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.455Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.577Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.607Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.618Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.638Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.692Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.712Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.748Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.786Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:40.820Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:38:41.487Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_7866154388499291482\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_7866154388499291482\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:39:48.230Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_7866154388499291482\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:39:48.754Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_16446666167473000412\" started. You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_16446666167473000412\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:19.104Z: JOB_MESSAGE_DETAILED: BigQuery export job progress: \"dataflow_job_16446666167473000412\" observed total of 1 exported files thus far.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:19.134Z: JOB_MESSAGE_BASIC: BigQuery export job finished: \"dataflow_job_16446666167473000412\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:25.171Z: JOB_MESSAGE_BASIC: Executing BigQuery import job \"dataflow_job_7866154388499291262\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_7866154388499291262\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:35.637Z: JOB_MESSAGE_BASIC: BigQuery import job \"dataflow_job_7866154388499291262\" done.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:35.996Z: JOB_MESSAGE_BASIC: Finished operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:36.053Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:36.079Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:36.094Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:36.120Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Close\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:36.158Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:36.184Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:38.985Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Read+Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow+Write log 1/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.035Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.088Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.116Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.135Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.155Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.186Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.212Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:39.273Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.259Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Read+Write log input/Write/WriteImpl/GroupByKey/GroupByWindow+Write log input/Write/WriteImpl/Extract\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.320Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/Extract.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.370Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.398Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.405Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.439Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.450Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.507Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsIter(Extract.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.557Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.762Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.812Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.868Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.910Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:42.954Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:43.007Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:43.823Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/PreFinalize\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:43.871Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:43.917Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:43.949Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:44.001Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(PreFinalize.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:44.059Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:46.038Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:46.518Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/FinalizeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:46.571Z: JOB_MESSAGE_DEBUG: Executing success step success27\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:46.676Z: JOB_MESSAGE_DETAILED: Cleaning up.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:46.721Z: JOB_MESSAGE_DEBUG: Starting worker pool teardown.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:40:46.746Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:42:56.350Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 1 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:42:56.390Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-25T21:42:56.448Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-25_14_36_16-8956075076107287369 is in state JOB_STATE_DONE\n"
     ]
    }
   ],
   "source": [
    "%run ssats_facility_treatment_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:--region not set; will default to us-central1. Future releases of Beam will require the user to set --region explicitly, or else have a default set via the gcloud tool. https://cloud.google.com/compute/docs/regions-zones\n",
      "/home/jupyter/Team-Foodies/venv/lib/python3.5/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:740: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
      "  kms_key=transform.kms_key))\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpinvs3bis', 'apache-beam==2.19.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/home/jupyter/Team-Foodies/venv/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpinvs3bis', 'apache-beam==2.19.0', '--no-deps', '--only-binary', ':all:', '--python-version', '35', '--implementation', 'cp', '--abi', 'cp35m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://beam_team_foodies/staging/transform-ssats-facility-information-df.1587934592.460788/apache_beam-2.19.0-cp35-cp35m-manylinux1_x86_64.whl in 1 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " createTime: '2020-04-26T20:56:38.254681Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2020-04-26_13_56_37-2869804457270373242'\n",
      " location: 'us-central1'\n",
      " name: 'transform-ssats-facility-information-df'\n",
      " projectId: 'first-planet-266901'\n",
      " stageStates: []\n",
      " startTime: '2020-04-26T20:56:38.254681Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2020-04-26_13_56_37-2869804457270373242]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobsDetail/locations/us-central1/jobs/2020-04-26_13_56_37-2869804457270373242?project=first-planet-266901\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-26_13_56_37-2869804457270373242 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:37.292Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2020-04-26_13_56_37-2869804457270373242.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:37.292Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2020-04-26_13_56_37-2869804457270373242. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:40.283Z: JOB_MESSAGE_DETAILED: Checking permissions granted to controller Service Account.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:40.812Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-4 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.311Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.352Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log 1/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.382Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write log input/Write/WriteImpl/GroupByKey: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.416Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.486Z: JOB_MESSAGE_DETAILED: Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.565Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.823Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.854Z: JOB_MESSAGE_DETAILED: Fusing consumer Format Data into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.881Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WriteBundles/WriteBundles into Read from BigQuery\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.905Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WriteBundles/WriteBundles into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.926Z: JOB_MESSAGE_DETAILED: Fusing consumer Write BQ table/WriteToBigQuery/NativeWrite into Format Data\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.955Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Pair into Write log input/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:41.978Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log input/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.004Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Reify into Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.031Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/Write into Write log input/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.063Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/GroupByKey/GroupByWindow into Write log input/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.088Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/Extract into Write log input/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.111Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Pair into Write log 1/Write/WriteImpl/WriteBundles/WriteBundles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.134Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn) into Write log 1/Write/WriteImpl/Pair\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.167Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Reify into Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.192Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/Write into Write log 1/Write/WriteImpl/GroupByKey/Reify\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.219Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow into Write log 1/Write/WriteImpl/GroupByKey/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.243Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/Extract into Write log 1/Write/WriteImpl/GroupByKey/GroupByWindow\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.270Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log input/Write/WriteImpl/InitializeWrite into Write log input/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.294Z: JOB_MESSAGE_DETAILED: Fusing consumer Write log 1/Write/WriteImpl/InitializeWrite into Write log 1/Write/WriteImpl/DoOnce/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.321Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.347Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.372Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.404Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.641Z: JOB_MESSAGE_DEBUG: Executing wait step start29\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.698Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.725Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.741Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.756Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.766Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-c...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.785Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.817Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.831Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/GroupByKey/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.881Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:56:42.904Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/GroupByKey/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2020-04-26_13_56_37-2869804457270373242 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:57:12.615Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:58:34.916Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:58:34.939Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.228Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/DoOnce/Read+Write log 1/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.276Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.290Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/DoOnce/Read+Write log input/Write/WriteImpl/InitializeWrite\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.298Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.338Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/DoOnce/Read.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.364Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/InitializeWrite.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.387Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.417Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.432Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.442Z: JOB_MESSAGE_BASIC: Executing operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.460Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.469Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.471Z: JOB_MESSAGE_BASIC: Finished operation Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.497Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.521Z: JOB_MESSAGE_BASIC: Executing operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.527Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.545Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.546Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.570Z: JOB_MESSAGE_BASIC: Finished operation Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.645Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.674Z: JOB_MESSAGE_DEBUG: Value \"Write log 1/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.703Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/FinalizeWrite/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.729Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/WriteBundles/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.757Z: JOB_MESSAGE_DEBUG: Value \"Write log input/Write/WriteImpl/PreFinalize/AsSingleton(InitializeWrite.out.0).output\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:02.781Z: JOB_MESSAGE_BASIC: Executing operation Read from BigQuery+Format Data+Write log input/Write/WriteImpl/WriteBundles/WriteBundles+Write log 1/Write/WriteImpl/WriteBundles/WriteBundles+Write BQ table/WriteToBigQuery/NativeWrite+Write log input/Write/WriteImpl/Pair+Write log input/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log input/Write/WriteImpl/GroupByKey/Reify+Write log input/Write/WriteImpl/GroupByKey/Write+Write log 1/Write/WriteImpl/Pair+Write log 1/Write/WriteImpl/WindowInto(WindowIntoFn)+Write log 1/Write/WriteImpl/GroupByKey/Reify+Write log 1/Write/WriteImpl/GroupByKey/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T20:59:03.662Z: JOB_MESSAGE_BASIC: BigQuery query issued as job: \"dataflow_job_15722618579461753300\". You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_15722618579461753300\".\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T21:00:09.600Z: JOB_MESSAGE_BASIC: BigQuery query completed, job : \"dataflow_job_15722618579461753300\"\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2020-04-26T21:00:09.913Z: JOB_MESSAGE_BASIC: BigQuery export job \"dataflow_job_13550800664838276597\" started. You can check its status with the bq tool: \"bq show -j --project_id=first-planet-266901 dataflow_job_13550800664838276597\".\n"
     ]
    }
   ],
   "source": [
    "%run ssats_facility_information_beam_dataflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.ssats_facility_information_Beam_DF as t2 \n",
    "LEFT JOIN ssats_modeled.ssats_facility_services_Beam_DF as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.ssats_facility_services_Beam_DF as t2 \n",
    "LEFT JOIN ssats_modeled.ssats_facility_information_Beam_DF as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.ssats_facility_services_Beam_DF as t2 \n",
    "LEFT JOIN ssats_modeled.ssats_facility_treatment_Beam_DF as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.ssats_facility_treatment_Beam_DF as t2 \n",
    "LEFT JOIN ssats_modeled.ssats_facility_services_Beam_DF as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.ssats_facility_treatment_Beam_DF as t2 \n",
    "LEFT JOIN ssats_modeled.ssats_facility_information_Beam_DF as t1 ON t2.CASEID = t1.CASEID\n",
    "WHERE t1.CASEID IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.ssats_facility_treatment_Beam_DF as t2 \n",
    "LEFT JOIN samhda_modeled.facility_treatment_Beam_DF as t1 ON t2.treatmt = t1.treatmt\n",
    "WHERE t1.treatmt IS NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f0_\n",
       "0    0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT COUNT(*) \n",
    "FROM ssats_modeled.ssats_facility_information_Beam_DF as t1 \n",
    "LEFT JOIN samhda_modeled.facility_information_Beam_DF as t2 ON t2.ownershp = t1.ownershp\n",
    "WHERE t1.ownershp IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check primary key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.ssats_facility_information_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.ssats_facility_information_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.ssats_facility_services_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.ssats_facility_services_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(distinct CASEID) from ssats_modeled.ssats_facility_treatment_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f0_\n",
       "0  70818"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "select count(*) from ssats_modeled.ssats_facility_treatment_Beam_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
